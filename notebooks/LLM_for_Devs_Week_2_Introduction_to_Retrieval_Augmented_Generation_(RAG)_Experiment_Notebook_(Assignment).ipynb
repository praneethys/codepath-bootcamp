{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 🥅 Goal\n",
        "\n",
        "In this notebook, we'll undertake a series of experiments to understand and explore the concepts and techniques behind Retrieval-Augmented Generation.\n",
        "\n",
        "To get started, make a copy of this notebook in your Google Drive and add your own code to capture your explorations and findings.\n",
        "\n",
        "As you work through the experiments, choose one topic that particularly interests you and dive deeper into it. Prepare a concise lightning talk on your chosen topic to present during Week 3.\n",
        "\n",
        "You do not have to deep dive into every experiement. Pick one and explore it more deeply. Prioritize finishing and deploying the assignment at the bottom of the notebook so you get more experience deploying a RAG-based LLM application.\n",
        "\n",
        "The goal is to gain a solid understanding of Retrieval-Augmented Generation and share your insights with others in Discord. Happy exploring!"
      ],
      "metadata": {
        "id": "v3M9HuEJwRmw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🧪 Experiment 1: How do I choose my text splitter?\n",
        "\n",
        "LangChain supports a number of [text splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/). As mentioned in the documentation, a recursive text splitting approach is the recommended way to start splitting text.\n",
        "\n",
        "`RecursiveCharacterTextSplitter` starts with a specified separator and if the resulting chunks are too large or not ideal, it recursively tries other separators.\n",
        "\n",
        "This adaptability ensures that the text is split into chunks that are more manageable and semantically meaningful, which is crucial for the effectiveness of RAG systems in retrieving relevant information.\n",
        "\n",
        "Due to its recursive nature, this splitter can ensure that the overlap between chunks is contextually relevant. This is important in RAG systems where the context of the information can significantly influence the quality of the generated responses. The overlapping chunks help maintain continuity and context, which can be lost in simpler splitting methods.\n"
      ],
      "metadata": {
        "id": "xOMuMf0GtB4k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 🧪 Experiment 2: How do I know what `chunk_size` and `chunk_overlap` to choose?\n",
        "\n",
        "[ChunkViz](https://chunkviz.up.railway.app/) is a nifty tool you can use to experiment with different `chunk_size` and `chunk_overlap`.\n",
        "\n",
        "Questions\n",
        "\n",
        "Experiment with different types of textual data (e.g., prose, HTML, Markdown, JSON).\n",
        "* What do you notice?\n",
        "* When do you notice a difference in performance based on how you set these parameters?\n",
        "\n",
        "Experiment with different types of non-texual data (e.g., audio, video, images)\n",
        "* How do you chunk non-textual data? What approaches and libaries exist?"
      ],
      "metadata": {
        "id": "Grd0XoS-tLgs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🧪 Experiment 3: How do I choose an embedding model?\n",
        "\n",
        "Research and experiment with different types of embedding models.\n",
        "\n",
        "* How much do they cost?\n",
        "* How do open source embedding models compare with paid embedding models?\n",
        "* Are there any differences between stability? Latency?\n",
        "* Do some embedding models perform better or a specific type of data?\n",
        "* What types of customization do different embedding models offer?"
      ],
      "metadata": {
        "id": "Cba2_Hw6y2sr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🧪 Experiment 4: How much time does using `CachedBackedEmbeddings` save you?\n",
        "\n",
        "Once the embedding is computed, it is stored in the cache for future use. This means that any subsequent request for the same text will retrieve the embedding directly from the cache, bypassing the computation step.\n",
        "\n",
        "* How do we measure that this caching is working?\n",
        "* Does it truly reduce the time for returning matching documents?\n",
        "* What happens if we ask the question a different way?\n",
        "* Do we still benefit from caching?\n",
        "* How much do we have to change the query for it to miss the cache?"
      ],
      "metadata": {
        "id": "LXctQVdvF_DY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🧪 Experiment 5: Improve the performance of `Mr-TD/RAG-PDF-QnA-ChatBot`\n",
        "\n",
        " Clone the [Hugging Face space](https://huggingface.co/spaces/Mr-TD/RAG-PDF-QnA-ChatBot/tree/main) `Mr-TD/RAG-PDF-QnA-ChatBot` referenced in the Week 2 lecture and try to make it more performant."
      ],
      "metadata": {
        "id": "Wm5xrvtq3jzI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🧪  Experiment 6: Implement RAG for Movie Recommendations using Haystack 2.0 or another library\n",
        "\n",
        "There are several libraries available for implementing a Retrieval-Augmented Generation (RAG) system. Each has its own strengths and weaknesses.\n",
        "\n",
        "[Hackstack 2.0](https://docs.haystack.deepset.ai/docs/intro) is known for being well-suited for production-quality large language models (LLMs), with features like:\n",
        "\n",
        "- Support for a variety of document stores, retrieval methods, and LLMs\n",
        "- Scalability to handle large datasets and high query volumes\n",
        "- Customizable pipelines for query processing, retrieval, and generation\n",
        "- Detailed documentation and active community support\n",
        "\n",
        "On the other hand, libraries like LangChain have a reputation for being easier to get started with for rapid prototyping, but may not be as robust for production-scale LLM applications.\n",
        "\n",
        "For this experiment, try implementing your movie recommendation RAG system using [Hackstack 2.0](https://docs.haystack.deepset.ai/docs/intro) or another library of your choice.\n",
        "\n",
        "As you work with the selected library, ask yourself the following questions:\n",
        "\n",
        "1. How easy is the library to set up and install?\n",
        "2. What is the quality and completeness of the documentation?\n",
        "3. Is the API well-designed, intuitive, and easy to learn?\n",
        "4. How active and supportive is the library's community?\n",
        "5. Does the library offer flexibility and customization options?\n",
        "6. How well does it integrate with your existing data and models?\n",
        "7. Are all essential aspects of usage covered in the documentation?"
      ],
      "metadata": {
        "id": "kcCvrV_jbbI6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 💻 Assignment: Deploy a RAG chatbot to production\n",
        "\n",
        "## Deploying your project as a chatbot\n",
        "\n",
        "Now that you have developed and tested your movie question answering system, the next step is to deploy it so that it can be accessed by users anywhere.\n",
        "\n",
        "You will deploy to Hugging Face Spaces using [Chainlit](https://chainlit.io), a powerful tool for deploying LLM projects as interactive web applications.\n"
      ],
      "metadata": {
        "id": "uIbiuy5KQzAI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting started with [Chainlit](https://chainlit.io) and Hugging Face Spaces\n",
        "\n",
        "[Chainlit](https://chainlit.io) is designed to make the deployment of machine learning models straightforward and accessible.\n",
        "\n",
        "Here are some resources to help you get started with deploying your project:\n",
        "\n",
        "1. **Explore Chainlit documentation**: Begin by exploring the [Chainlit Documentation](https://chainlit.io/docs). This resource provides comprehensive guidance on how to use Chainlit, from installation to deployment.\n",
        "\n",
        "2. **Learn about Hugging Face Spaces**: Learn more about Hugging Face Spaces, which is a platform for hosting ML models and apps, by visiting [Hugging Face Spaces](https://huggingface.co/spaces). This platform integrates seamlessly with Chainlit, making it an ideal choice for deploying your chatbot.\n",
        "\n",
        "3. **Browse example projects**: It can also be helpful to look at example projects on Hugging Face Spaces to see how others have structured their deployments. This can provide inspiration and practical insights for your own deployment.\n"
      ],
      "metadata": {
        "id": "IMQZw_u-Q14P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deploying your chatbot\n",
        "\n",
        "To deploy your RAG question answering system as a chatbot, follow these steps:\n",
        "\n",
        "1. **Prepare your project** Create a folder for your project and ensure that it is organized and well-documented. This includes having clear code comments, a requirements.txt file for dependencies, and a README for instructions.\n",
        "\n",
        "2. **Set up Chainlit** Install Chainlit on your local machine or development environment.\n",
        "\n",
        "3. **Build and deploy your chatbot** Using the above resources, figure out how to deploy your chatbot to a Hugging Face Space."
      ],
      "metadata": {
        "id": "uBofPZoyQ5NL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📝 Submission\n",
        "\n",
        "Submit your experiment notebook for Week 2 using the form [here](https://docs.google.com/forms/d/1l935d2L3YN3Kj3ovNf3CKWB2EyxvDMkYY_sYte-NYWI/edit).\n",
        "\n",
        "Please make sure sharing permissions are turned on for everyone with the link.\n",
        "\n",
        "Note: In Week 3, we'll do lightning presentations where each student will share the results of their experiment.\n"
      ],
      "metadata": {
        "id": "OZ1z3fjmxg7H"
      }
    }
  ]
}